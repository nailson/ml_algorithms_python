{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  actual_status predicted_status\n0      Positive         Positive\n1      Positive         Negative\n2      Negative         Positive\n3      Negative         Negative\n4      Negative         Positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>actual_status</th>\n      <th>predicted_status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Positive</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Positive</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Negative</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Negative</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Negative</td>\n      <td>Positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Toy example for metrics evaluation\n",
    "data = pd.DataFrame(columns=['actual_status', 'predicted_status'],\n",
    "                   data=[\n",
    "                       ['Positive', 'Positive'],\n",
    "                       ['Positive', 'Negative'],\n",
    "                       ['Negative', 'Positive'],\n",
    "                       ['Negative', 'Negative'],\n",
    "                       ['Negative', 'Positive'],\n",
    "                       ['Negative', 'Negative'],\n",
    "                       ['Negative', 'Positive'],\n",
    "                       ['Negative', 'Negative'],\n",
    "                       ['Positive', 'Negative'],\n",
    "                       ['Positive', 'Negative'],\n",
    "                       ['Positive', 'Negative'],\n",
    "                       ['Positive', 'Positive'],\n",
    "                       ['Positive', 'Negative'],\n",
    "                       ['Negative', 'Positive'],\n",
    "                       ['Negative', 'Negative'],\n",
    "                       ['Negative', 'Negative'],\n",
    "                       ['Negative', 'Negative']\n",
    "                   ])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Negative    10\nPositive     7\nName: actual_status, dtype: int64"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "data.actual_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{(True, True): 2, (True, False): 5, (False, True): 4, (False, False): 6}"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "from collections import Counter\n",
    "confusion_dict = dict(Counter((row.actual_status == 'Positive', row.predicted_status == 'Positive') for index, row in data.iterrows()))\n",
    "\n",
    "confusion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'TP': 2, 'FP': 6, 'FN': 5}"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "confusion_dict['TP'] = confusion_dict.pop((True, True))\n",
    "confusion_dict['FP'] = confusion_dict.pop((False, True))\n",
    "confusion_dict['FN'] = confusion_dict.pop((True, False))\n",
    "confusion_dict['FP'] = confusion_dict.pop((False, False))\n",
    "\n",
    "confusion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{(True, True): 2, (True, False): 5, (False, True): 4, (False, False): 6}"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "def calculate_confusion_dict(y_real, y_pred):\n",
    "    df = pd.DataFrame({'y_real':y_real, 'y_pred':y_pred})\n",
    "    confusion_dict = dict(Counter((row.y_real == 'Positive', \n",
    "                                   row.y_pred == 'Positive') for index, row in df.iterrows()))\n",
    "    return  confusion_dict\n",
    "\n",
    "calculate_confusion_dict(data.actual_status, data.predicted_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.47058823529411764"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "def accuracy(y_real, y_pred):\n",
    "    confusion_dict = calculate_confusion_dict(y_real, y_pred)\n",
    "    TP = confusion_dict[(True, True)]\n",
    "    TN = confusion_dict[(False, False)]\n",
    "    return (TP + TN) / len(y_real)\n",
    "\n",
    "accuracy(data.actual_status, data.predicted_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and F1-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3333333333333333"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "def precision(y_real, y_pred):\n",
    "    confusion_dict = calculate_confusion_dict(y_real, y_pred)\n",
    "    TP = confusion_dict[(True, True)]\n",
    "    FP = confusion_dict[(False, True)]\n",
    "    return TP/(TP + FP)\n",
    "\n",
    "precision(data.actual_status, data.predicted_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.2857142857142857"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "def recall(y_real, y_pred):\n",
    "    confusion_dict = calculate_confusion_dict(y_real, y_pred)\n",
    "    TP = confusion_dict[(True, True)]\n",
    "    FN = confusion_dict[(True, False)]\n",
    "    return TP/(TP + FN)\n",
    "\n",
    "recall(data.actual_status, data.predicted_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.0"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "def harmonic_mean(numbers_list):\n",
    "    inverse_sum = sum([a**-1 for a in numbers_list])\n",
    "    list_size = len(numbers_list)\n",
    "    return (inverse_sum/list_size)**-1\n",
    "\n",
    "harmonic_mean([1, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3076923076923077"
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "def f1_measure(y_real, y_pred):\n",
    "    prec = precision(y_real, y_pred)\n",
    "    rec = recall(y_real, y_pred)\n",
    "    return harmonic_mean([prec, rec])\n",
    "\n",
    "f1_measure(data.actual_status, data.predicted_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.26315789473684215"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "def fbeta_score(y_real, y_pred, beta=0.5):\n",
    "    prec = precision(y_real, y_pred)\n",
    "    rec = recall(y_real, y_pred)\n",
    "    numerator = (1 + beta**2) * (prec * rec)\n",
    "    denominator = (beta * prec) + rec\n",
    "    return numerator / denominator\n",
    "\n",
    "fbeta_score(data.actual_status, data.predicted_status)"
   ]
  }
 ]
}